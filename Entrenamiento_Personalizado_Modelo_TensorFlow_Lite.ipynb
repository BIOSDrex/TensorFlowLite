{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "a6eHflisAx3l",
        "9UGfE7w8ApAu",
        "pgF4VVdM1eLi"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**# Script Para Entrenamineto del Modelo Personalizado**"
      ],
      "metadata": {
        "id": "n_O5BrI8-L6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Este cuaderno de google-colab, muestra una guia de entrenamiento de modelos personalizados para la detccion de objetos utilizando el framework TensorFlow "
      ],
      "metadata": {
        "id": "AFobBIYmAvd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 1: Recolección de imagenes y etiquetado con Labelimg"
      ],
      "metadata": {
        "id": "a6eHflisAx3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para la Recoleccion de imagenes, se estima que sean entre 800 y 1000 imaganes para que el entrenamiento sea eficiente, luego etiquetar cada una de estas imagenes con el programa (labelimg), este procedimiento se eplica en la guia paso a paso."
      ],
      "metadata": {
        "id": "CGz_FQ8aCy5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 2: Instalacion de las Dependencias de Deteccion de Objetos de TensorFlow"
      ],
      "metadata": {
        "id": "9UGfE7w8ApAu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ypWGYdPlLRUN"
      },
      "outputs": [],
      "source": [
        "# Clonamos el repositorio de los modelos de tensorflow desde GitHub !!!\n",
        "!git clone --depth 1 https://github.com/tensorflow/models"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Se copian los archivos de instalacion a la carpeta de los mo\n",
        "%%bash\n",
        "cd models/research/\n",
        "protoc object_detection/protos/*.proto --python_out=."
      ],
      "metadata": {
        "id": "fXMhWfRTBiqK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Modificar setup.py si desea instalar otra version de TensorFlow del repositorio\n",
        "import re\n",
        "with open('/content/models/research/object_detection/packages/tf2/setup.py') as f:\n",
        "    s = f.read()\n",
        "\n",
        "with open('/content/models/research/setup.py', 'w') as f:\n",
        "    # Establecer la Ruta\n",
        "    s = re.sub('tf-models-official>=2.5.1',\n",
        "               'tf-models-official==2.8.0', s)\n",
        "    f.write(s)"
      ],
      "metadata": {
        "id": "1aQJf7LXDCX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar API de deteccion de objetos\n",
        "!pip install /content/models/research/\n",
        "\n",
        "# Instalar TensorFlow Version 2.8.0\n",
        "!pip install tensorflow==2.8.0"
      ],
      "metadata": {
        "id": "eqDf_azODzQ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecuta un Archivo de prueba, para verificar que todo funcione correctamente \n",
        "!python /content/models/research/object_detection/builders/model_builder_tf2_test.py"
      ],
      "metadata": {
        "id": "4fhh0-YXEZOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 3: Cargar Conjunto de Datos (Imagenes y Etiquetado)"
      ],
      "metadata": {
        "id": "mD6nRZ_FGww5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para este paso se cargara el conjunto de datos para el entrenamiento con TensorFlow, Primero se debe comprimir todas las imágenes de entrenamiento y archivos XML en una sola carpeta llamada \"imágenes.zip\". Los archivos deben estar directamente dentro de la carpeta zip (sin carpetas añidadas) como se muestra a continuación:\n",
        "\n",
        "```\n",
        "images.zip\n",
        "-- img1.jpg\n",
        "-- img1.xml\n",
        "-- img2.jpg\n",
        "-- img2.xml\n",
        "...\n",
        "```"
      ],
      "metadata": {
        "id": "5VcwN4PbHDPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.1 Subir Imagenes \n",
        "Para subir las imagenes hay dos opciones "
      ],
      "metadata": {
        "id": "loKErP7qJx-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Opción 1. Sube através de Google Colab**\n",
        "\n",
        "Haciendo clic en el icono \"Archivos\" en el lado izquierdo del navegador y luego en el icono \"Cargar en el almacenamiento\". Seleccione la carpeta zip desde su pc para cargarla.\n",
        "\n",
        "![cature.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "_SDaGihGKwot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Opción 2. Copiar desde Google Drive**\n",
        "\n",
        "Montar la unidad en esta sesión de Colab y copiarlas en el sistema de archivos. Esta opción funciona bien si desea cargar las imágenes de antemano para que no tenga que esperar a que se carguen cada vez que reinicie este Colab.\n",
        "\n",
        "Primero, cargue el archivo \"images.zip\" en su Google Drive y tome nota de la carpeta en la que las cargó. Reempláce \"/content/gdrive/MyDrive/path/to/images.zip\" por la ruta a su archivo zip. (Por ejemplo, si subí el archivo zip a la carpeta llamada \"entrenamiento\", por lo que lo usaría \"/content/gdrive/MyDrive/entrenamiento/images.zip\" para la ruta). Luego, ejecute el siguiente bloque de código para montar su Google Drive a Colab.\n"
      ],
      "metadata": {
        "id": "OhGIopqJMZuZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!cp /content/gdrive/MyDrive/path/to/images.zip /content"
      ],
      "metadata": {
        "id": "-jNlz4XdHB4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 Dividir las Imagenes en tres carpetas: Entrenamiento, Validacion, y Prueba\n",
        "\n",
        "Ya sea que haya utilizado la opción 1 o 2 debería poder hacer clic en el icono de archivos a la izquierda y ver \"images.zip\". Ahora que el conjunto de datos está cargado, vamos a descomprimirlo y crear tres carpetas para contener las imágenes, entrenamiento, validacion y prueba corriendo el siguiente codigo. "
      ],
      "metadata": {
        "id": "4U1tPFO0QoCt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/images\n",
        "!unzip -q images.zip -d /content/images/all\n",
        "!mkdir /content/images/train; mkdir /content/images/validation; mkdir /content/images/test"
      ],
      "metadata": {
        "id": "fhdHR1vbSmYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Se dividen las imágenes en conjuntos de entrenamiento, validación y prueba.\n",
        "\n",
        "Este script movera aleatoriamente el 80 % de las imágenes a la carpeta \"entrenamiento\", el 10 % a la carpeta \"validación\" y el ultimo 10 % a la carpeta \"prueba\".\n"
      ],
      "metadata": {
        "id": "9svHM65Wasyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/BIOSDrex/TensorFlowLite/master/scripts/train_val_test_split.py\n",
        "!python train_val_test_split.py"
      ],
      "metadata": {
        "id": "_0U3zyunb9IA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 Creación de los archivos Labelmap y TFRecords \n",
        "\n",
        "Se necesita crear un mapa de etiquetas (labelmap) y convertir las imágenes a un formato de archivo de datos llamado TFRecords, que TensorFlow usa para el entrenamiento. Antes de ejecutarlos, se necesita definir un mapa de etiquetas para las clases.\n",
        "\n",
        "La siguiente sección de código creará un archivo \"labelmap.txt\" que contiene una lista de las clases personalizadas. solo tiene que Reemplazar el texto class1, class2, class3 con sus propias clases (ejemplo, persona, perro, gato etc), luego ejecuta el codigo."
      ],
      "metadata": {
        "id": "aDh8-Oj1cU0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### creacion del archivo \"labelmap.txt con las clases\" \n",
        "%%bash\n",
        "cat <<EOF >> /content/labelmap.txt\n",
        "class1\n",
        "class2\n",
        "class3\n",
        "EOF"
      ],
      "metadata": {
        "id": "9BvRuSKZeMR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargue y ejecute los scripts de conversión de datos del repositorio de GitHub, solo haciendo clic en reproducir en las siguientes tres secciones de código Crearán archivos TFRecord para los conjuntos de datos de entrenamiento y validación, así como un archivo 'labelmap.pbtxt' que contiene el mapa de etiquetas en un formato diferente."
      ],
      "metadata": {
        "id": "-OZNIb4Ufa-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# conversion\n",
        "! wget https://raw.githubusercontent.com/BIOSDrex/TensorFlowLite/master/scripts/create_csv.py\n",
        "! wget https://raw.githubusercontent.com/BIOSDrex/TensorFlowLite/master/scripts/create_tfrecord.py"
      ],
      "metadata": {
        "id": "3aie6UfSf4Rp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creación de los archivos CSV y TFRecords \n",
        "!python3 create_csv.py\n",
        "!python3 create_tfrecord.py --csv_input=images/train_labels.csv --labelmap=labelmap.txt --image_dir=images/train --output_path=train.tfrecord\n",
        "!python3 create_tfrecord.py --csv_input=images/validation_labels.csv --labelmap=labelmap.txt --image_dir=images/validation --output_path=val.tfrecord"
      ],
      "metadata": {
        "id": "BR2EojnFf4mZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_record_fname = '/content/train.tfrecord'\n",
        "val_record_fname = '/content/val.tfrecord'\n",
        "label_map_pbtxt_fname = '/content/labelmap.pbtxt'"
      ],
      "metadata": {
        "id": "tHdiFQldgs6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 4: Configuración del Entrenamiento"
      ],
      "metadata": {
        "id": "fSv9PL2dhRyZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta sección, se configurara el modelo y el entrenamiento. si desea correr otro modelo diferente a los que estan es el scripts solo entre a este link y escoja el modelo que quiere usar [TensorFlow Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md). . Cada modelo también viene con un archivo de configuración que apunta a las ubicaciones de los archivos, establece parámetros de entrenamiento (como la tasa de aprendizaje y el número total de pasos de entrenamiento) y más. \n",
        "\n",
        "La primera sección del código enumera algunos modelos disponibles en TF2 \n",
        "\n",
        "Configure la variable \"chosen_model\" para que coincida con el nombre del modelo con el que le gustaría entrenar. Actualmente está configurado para usar el popular modelo \"ssd-mobilenet-v2\" si no desea cambiar otro modelo solo ejecute el scrpts sin modificar nada. "
      ],
      "metadata": {
        "id": "AEm4Ty1FiCbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Elegir el Modelo\n",
        "chosen_model = 'ssd-mobilenet-v2'\n",
        "\n",
        "MODELS_CONFIG = {\n",
        "    'ssd-mobilenet-v2': {\n",
        "        'model_name': 'ssd_mobilenet_v2_320x320_coco17_tpu-8',\n",
        "        'base_pipeline_file': 'ssd_mobilenet_v2_320x320_coco17_tpu-8.config',\n",
        "        'pretrained_checkpoint': 'ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz',\n",
        "    },\n",
        "    'efficientdet-d0': {\n",
        "        'model_name': 'efficientdet_d0_coco17_tpu-32',\n",
        "        'base_pipeline_file': 'ssd_efficientdet_d0_512x512_coco17_tpu-8.config',\n",
        "        'pretrained_checkpoint': 'efficientdet_d0_coco17_tpu-32.tar.gz',\n",
        "    },\n",
        "    'ssd-mobilenet-v2-fpnlite-320': {\n",
        "        'model_name': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8',\n",
        "        'base_pipeline_file': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.config',\n",
        "        'pretrained_checkpoint': 'ssd_mobilenet_v2_fpnlite_320x320_coco17_tpu-8.tar.gz',\n",
        "    },\n",
        "    # The centernet model isn't working as of 9/10/22\n",
        "    #'centernet-mobilenet-v2': {\n",
        "    #    'model_name': 'centernet_mobilenetv2fpn_512x512_coco17_od',\n",
        "    #    'base_pipeline_file': 'pipeline.config',\n",
        "    #    'pretrained_checkpoint': 'centernet_mobilenetv2fpn_512x512_coco17_od.tar.gz',\n",
        "    #}\n",
        "}\n",
        "\n",
        "model_name = MODELS_CONFIG[chosen_model]['model_name']\n",
        "pretrained_checkpoint = MODELS_CONFIG[chosen_model]['pretrained_checkpoint']\n",
        "base_pipeline_file = MODELS_CONFIG[chosen_model]['base_pipeline_file']"
      ],
      "metadata": {
        "id": "_nvhEnIPjzdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargue el archivo del modelo preentrenado y el archivo de configuración, ejecutando la siguiente sección de codigo."
      ],
      "metadata": {
        "id": "7uAS_hi1kvel"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creacion de la carpeta \"mymodel\" para los pesos previamente entrenados\n",
        "%mkdir /content/models/mymodel/\n",
        "%cd /content/models/mymodel/\n",
        "\n",
        "# Descargar los pesos del modelo Pre-entrenado \n",
        "import tarfile\n",
        "download_tar = 'http://download.tensorflow.org/models/object_detection/tf2/20200711/' + pretrained_checkpoint\n",
        "!wget {download_tar}\n",
        "tar = tarfile.open(pretrained_checkpoint)\n",
        "tar.extractall()\n",
        "tar.close()\n",
        "\n",
        "# Descargar el archivo de configuración para el entrenamiento \n",
        "download_config = 'https://raw.githubusercontent.com/tensorflow/models/master/research/object_detection/configs/tf2/' + base_pipeline_file\n",
        "!wget {download_config}"
      ],
      "metadata": {
        "id": "ywi5Uh7yk_F1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "despues de descargar el modelo y archivo de configuración, se modifica el archivo de configuración con algunos parámetros de entrenamiento. Las siguientes variables se utilizan para controlar los pasos de entrenamiento:\n",
        "\n",
        "num_steps: la cantidad total de pasos que se usarán para entrenar el modelo. Un buen número para empezar es 20.000 pasos. Cuantos más pasos, más tiempo llevará el entrenamiento.\n",
        "\n",
        "batch_size: la cantidad de imágenes que se usarán por paso de entrenamiento. Con las GPU utilizadas en las instancias de Colab, 16 es un buen número para los modelos SSD y 4 es bueno para los modelos EfficientDet."
      ],
      "metadata": {
        "id": "_F92_a2SmKwW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parametros del Entrenamiento\n",
        "num_steps = 20000\n",
        "\n",
        "if chosen_model == 'efficientdet-d0':\n",
        "  batch_size = 4\n",
        "else:\n",
        "  batch_size = 16"
      ],
      "metadata": {
        "id": "HRP1-_DwnOLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtener el numero de clases\n",
        "pipeline_fname = '/content/models/mymodel/' + base_pipeline_file\n",
        "fine_tune_checkpoint = '/content/models/mymodel/' + model_name + '/checkpoint/ckpt-0'\n",
        "\n",
        "def get_num_classes(pbtxt_fname):\n",
        "    from object_detection.utils import label_map_util\n",
        "    label_map = label_map_util.load_labelmap(pbtxt_fname)\n",
        "    categories = label_map_util.convert_label_map_to_categories(\n",
        "        label_map, max_num_classes=90, use_display_name=True)\n",
        "    category_index = label_map_util.create_category_index(categories)\n",
        "    return len(category_index.keys())\n",
        "num_classes = get_num_classes(label_map_pbtxt_fname)\n",
        "print('Total classes:', num_classes)"
      ],
      "metadata": {
        "id": "L0VlhdBhndJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, se reescribe el archivo de configuración para usar los parámetros de entrenamiento modificados. La siguiente sección de código los reemplazará automáticamente y lo guardará como un archivo \"pipeline_file.config\" personalizado."
      ],
      "metadata": {
        "id": "SBWdptMMo2o9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creacion del archivo de configuración personalizado\n",
        "import re\n",
        "\n",
        "%cd /content/models/mymodel\n",
        "print('writing custom configuration file')\n",
        "\n",
        "with open(pipeline_fname) as f:\n",
        "    s = f.read()\n",
        "with open('pipeline_file.config', 'w') as f:\n",
        "    \n",
        "    s = re.sub('fine_tune_checkpoint: \".*?\"',\n",
        "               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n",
        "    \n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n",
        "    s = re.sub(\n",
        "        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")', 'input_path: \"{}\"'.format(val_record_fname), s)\n",
        "\n",
        "    s = re.sub(\n",
        "        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n",
        "\n",
        "    s = re.sub('batch_size: [0-9]+',\n",
        "               'batch_size: {}'.format(batch_size), s)\n",
        "\n",
        "    s = re.sub('num_steps: [0-9]+',\n",
        "               'num_steps: {}'.format(num_steps), s)\n",
        "    \n",
        "    s = re.sub('num_classes: [0-9]+',\n",
        "               'num_classes: {}'.format(num_classes), s)\n",
        "\n",
        "    s = re.sub(\n",
        "        'fine_tune_checkpoint_type: \"classification\"', 'fine_tune_checkpoint_type: \"{}\"'.format('detection'), s)\n",
        "    \n",
        "    if chosen_model == 'ssd-mobilenet-v2':\n",
        "      s = re.sub('learning_rate_base: .8',\n",
        "                 'learning_rate_base: .08', s)\n",
        "      \n",
        "      s = re.sub('warmup_learning_rate: 0.13333',\n",
        "                 'warmup_learning_rate: .026666', s)\n",
        "    \n",
        "    if chosen_model == 'efficientdet-d0':\n",
        "      s = re.sub('keep_aspect_ratio_resizer', 'fixed_shape_resizer', s)\n",
        "      s = re.sub('pad_to_max_dimension: true', '', s)\n",
        "      s = re.sub('min_dimension', 'height', s)\n",
        "      s = re.sub('max_dimension', 'width', s)\n",
        "\n",
        "    f.write(s)\n"
      ],
      "metadata": {
        "id": "4idyqg-RpWCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, configuramos las ubicaciones del archivo y el directorio de salida del modelo como variables para cuando llamemos al comando de entrenamiento."
      ],
      "metadata": {
        "id": "LOJq5ycIrrMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ruta al archivo de configuración personalizado\n",
        "pipeline_file = '/content/models/mymodel/pipeline_file.config'\n",
        "model_dir = '/content/training/'"
      ],
      "metadata": {
        "id": "_5rTxTBwsHp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 5: Entrenamiento del modelo de deteccion personalizado."
      ],
      "metadata": {
        "id": "11UO4J8ztNhe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para entrenar nuestro modelo de detección de objetos, Antes de comenzar a entrenar, se carga una sesión de TensorBoard para monitorear el progreso del entrenamiento. Ejecute la siguiente sección de código y aparecerá una sesión de TensorBoard en el navegador. Todavía no mostrará nada, porque no se ha comenzado a entrenar. Una vez que comience el entrenamiento, regrese y haga clic en el botón Actualizar para ver la pérdida general del modelo."
      ],
      "metadata": {
        "id": "2W5cOqmWt06v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar graficas con TensorBoard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir '/content/training/train'"
      ],
      "metadata": {
        "id": "JCEdGRszuQf4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El entrenamiento del modelo se realiza mediante el siguiente script de la API de detección de objetos de TF. el tiempo de entrenamiento tomará de 1 a 2 horas, según el modelo, el tamaño del lote y la cantidad de pasos."
      ],
      "metadata": {
        "id": "tEawiY9Ku6JF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correr Entrenamiento\n",
        "!python /content/models/research/object_detection/model_main_tf2.py \\\n",
        "    --pipeline_config_path={pipeline_file} \\\n",
        "    --model_dir={model_dir} \\\n",
        "    --alsologtostderr \\\n",
        "    --num_train_steps={num_steps} \\\n",
        "    --sample_1_of_n_eval_examples=1"
      ],
      "metadata": {
        "id": "PPB-0B6KvcM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Si desea detener el entrenamiento antes de tiempo cuando ya la perdida sea bastante baja, haga clic con el botón derecho en el bloque de código y seleccione \"Interrumpir ejecución\". De lo contrario, el entrenamiento se detendrá cuando termine el numero de pasos asignados."
      ],
      "metadata": {
        "id": "9TKb1wWJv40s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 6: Convertir el Modelo a TensorFlow Lite"
      ],
      "metadata": {
        "id": "EYeYeaCvwc9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego de termine el entrenamineto, necesitamos exportar el gráfico del modelo (un archivo que contiene información sobre la arquitectura y los pesos) a un formato compatible con TensorFlow Lite."
      ],
      "metadata": {
        "id": "hwzonPsvw16P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# conversion del modelo\n",
        "!mkdir /content/custom_model_lite\n",
        "output_directory = '/content/custom_model_lite'\n",
        "\n",
        "last_model_path = '/content/training'\n",
        "\n",
        "!python /content/models/research/object_detection/export_tflite_graph_tf2.py \\\n",
        "    --trained_checkpoint_dir {last_model_path} \\\n",
        "    --output_directory {output_directory} \\\n",
        "    --pipeline_config_path {pipeline_file}\n"
      ],
      "metadata": {
        "id": "Nm4chQsmxhpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, se toma el gráfico exportado y usaremos el módulo 'TFLiteConverter' para convertirlo al formato '.tflite' "
      ],
      "metadata": {
        "id": "FZWZFW5cx9gu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convierte a .tflite\n",
        "import tensorflow as tf\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model('/content/custom_model_lite/saved_model')\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open('/content/custom_model_lite/detect.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "metadata": {
        "id": "lYsXPhpMyNcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 7: Prueba del modelo personalizado y Calculo de la precisión media(mAP)."
      ],
      "metadata": {
        "id": "th1zqe5N0EvL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Despues de convertir el modelo al formato TFLite. se toma la carpeta con las imagenes de prueba y se corre el siguiente script para ver como se comporta el modelo"
      ],
      "metadata": {
        "id": "bGZdED6r0VX2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 Inferencia con las imagenes de prueba\n",
        "\n",
        "El siguiente código define una función para ejecutar la inferencia en imágenes de prueba. Carga las imágenes, carga el modelo y el mapa de etiquetas, ejecuta el modelo en cada imagen y muestra el resultado. Opcionalmente, también guarda los resultados de detección como archivos de texto para que podamos usarlos para calcular la puntuación de mAP del modelo."
      ],
      "metadata": {
        "id": "pgF4VVdM1eLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Script de prueba\n",
        "\n",
        "# Import packages\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import sys\n",
        "import glob\n",
        "import random\n",
        "import importlib.util\n",
        "from tensorflow.lite.python.interpreter import Interpreter\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "def tflite_detect_images(modelpath, imgpath, lblpath, min_conf=0.5, num_test_images=10, savepath='/content/results', txt_only=False):\n",
        "\n",
        "  images = glob.glob(imgpath + '/*.jpg') + glob.glob(imgpath + '/*.JPG') + glob.glob(imgpath + '/*.png') + glob.glob(imgpath + '/*.bmp')\n",
        "\n",
        "  \n",
        "  with open(lblpath, 'r') as f:\n",
        "      labels = [line.strip() for line in f.readlines()]\n",
        "\n",
        "  \n",
        "  interpreter = Interpreter(model_path=modelpath)\n",
        "  interpreter.allocate_tensors()\n",
        "\n",
        "  \n",
        "  input_details = interpreter.get_input_details()\n",
        "  output_details = interpreter.get_output_details()\n",
        "  height = input_details[0]['shape'][1]\n",
        "  width = input_details[0]['shape'][2]\n",
        "\n",
        "  float_input = (input_details[0]['dtype'] == np.float32)\n",
        "\n",
        "  input_mean = 127.5\n",
        "  input_std = 127.5\n",
        "\n",
        "\n",
        "  images_to_test = random.sample(images, num_test_images)\n",
        "\n",
        "\n",
        "  for image_path in images_to_test:\n",
        "\n",
        "      image = cv2.imread(image_path)\n",
        "      image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "      imH, imW, _ = image.shape \n",
        "      image_resized = cv2.resize(image_rgb, (width, height))\n",
        "      input_data = np.expand_dims(image_resized, axis=0)\n",
        "\n",
        "      if float_input:\n",
        "          input_data = (np.float32(input_data) - input_mean) / input_std\n",
        "\n",
        "      interpreter.set_tensor(input_details[0]['index'],input_data)\n",
        "      interpreter.invoke()\n",
        "\n",
        "\n",
        "      boxes = interpreter.get_tensor(output_details[1]['index'])[0] \n",
        "      classes = interpreter.get_tensor(output_details[3]['index'])[0] \n",
        "      scores = interpreter.get_tensor(output_details[0]['index'])[0] \n",
        "\n",
        "      detections = []\n",
        "\n",
        "      for i in range(len(scores)):\n",
        "          if ((scores[i] > min_conf) and (scores[i] <= 1.0)):\n",
        "\n",
        "\n",
        "              ymin = int(max(1,(boxes[i][0] * imH)))\n",
        "              xmin = int(max(1,(boxes[i][1] * imW)))\n",
        "              ymax = int(min(imH,(boxes[i][2] * imH)))\n",
        "              xmax = int(min(imW,(boxes[i][3] * imW)))\n",
        "              \n",
        "              cv2.rectangle(image, (xmin,ymin), (xmax,ymax), (10, 255, 0), 2)\n",
        "\n",
        "              # label\n",
        "              object_name = labels[int(classes[i])] \n",
        "              label = '%s: %d%%' % (object_name, int(scores[i]*100)) \n",
        "              labelSize, baseLine = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2) \n",
        "              label_ymin = max(ymin, labelSize[1] + 10) \n",
        "              cv2.rectangle(image, (xmin, label_ymin-labelSize[1]-10), (xmin+labelSize[0], label_ymin+baseLine-10), (255, 255, 255), cv2.FILLED) \n",
        "              cv2.putText(image, label, (xmin, label_ymin-7), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 0), 2) \n",
        "\n",
        "              detections.append([object_name, scores[i], xmin, ymin, xmax, ymax])\n",
        "\n",
        "      \n",
        "      if txt_only == False:\n",
        "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
        "        plt.figure(figsize=(12,16))\n",
        "        plt.imshow(image)\n",
        "        plt.show()\n",
        "      \n",
        "\n",
        "      elif txt_only == True:\n",
        "\n",
        "        \n",
        "        image_fn = os.path.basename(image_path)      \n",
        "        base_fn, ext = os.path.splitext(image_fn)\n",
        "        txt_result_fn = base_fn +'.txt'\n",
        "        txt_savepath = os.path.join(savepath, txt_result_fn)\n",
        "\n",
        "        \n",
        "        with open(txt_savepath,'w') as f:\n",
        "            for detection in detections:\n",
        "                f.write('%s %.4f %d %d %d %d\\n' % (detection[0], detection[1], detection[2], detection[3], detection[4], detection[5]))\n",
        "\n",
        "  return"
      ],
      "metadata": {
        "id": "AgD2YPq92Hci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El siguiente bloque establece las rutas a las imágenes y modelos de prueba y luego ejecuta la función de inferencia."
      ],
      "metadata": {
        "id": "ds3tAETc3yCo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# variables y carpetas del modelo\n",
        "PATH_TO_IMAGES='/content/images/test'   # ruta a la carpeta imagenes de prueba\n",
        "PATH_TO_MODEL='/content/custom_model_lite/detect.tflite'   # ruta a la carpeta donde se encuentra el modelo.tflite\n",
        "PATH_TO_LABELS='/content/labelmap.txt'   # ruta a la carpeta donde se encuentra el archivo.txt\n",
        "min_conf_threshold=0.5   # umbral de confianza\n",
        "images_to_test = 10  # numero de imagenes de prueba\n",
        "\n",
        "# correr la funcion de inferencia\n",
        "tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test)"
      ],
      "metadata": {
        "id": "9QKZ-wnC4FFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2 Calcular (mAP)\n",
        "\n",
        "precisión media (mAP). Cuanto mayor sea la puntuación de mAP, mejor será su modelo para detectar objetos"
      ],
      "metadata": {
        "id": "NVtZtAbR4-8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "git clone https://github.com/Cartucho/mAP /content/mAP\n",
        "cd /content/mAP\n",
        "rm input/detection-results/* \n",
        "rm input/ground-truth/* \n",
        "rm input/images-optional/* \n",
        "wget https://raw.githubusercontent.com/BIOSDrex/TensorFlowLite/master/scripts/calculate_map_cartucho.py"
      ],
      "metadata": {
        "id": "qgY_r-kD5xTF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, copiaremos las imágenes y los datos de anotación de la carpeta de prueba. Estos se utilizarán como los \"datos de verdad\" con los que se compararán los resultados de detección del modelo."
      ],
      "metadata": {
        "id": "xcP1VJqf7QJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/images/test/* /content/mAP/input/images-optional # Copiar imagenes y archivos xml\n",
        "!mv /content/mAP/input/images-optional/*.xml /content/mAP/input/ground-truth/  # Mover archivos xml"
      ],
      "metadata": {
        "id": "kewlqg2952BP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/mAP/scripts/extra/convert_gt_xml.py"
      ],
      "metadata": {
        "id": "FoIB1yyB77k9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configurado los datos, los resultados de la detección se compararán con los datos de verdad para calcular la precisión del modelo en mAP. Solo toca ejecutar el siguiente bloque de código."
      ],
      "metadata": {
        "id": "pBTdcp_U8VIX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Configurar variables para ejecutar la inferencia\n",
        "PATH_TO_IMAGES='/content/images/test'   # ruta de la carpeta de las imagenes de prueba\n",
        "PATH_TO_MODEL='/content/custom_model_lite/detect.tflite'   # ruta de la carpeta donde esta el modelo.tflite\n",
        "PATH_TO_LABELS='/content/labelmap.txt'   # ruta de la carpeta donde esta el archivo.txt\n",
        "PATH_TO_RESULTS='/content/mAP/input/detection-results' # carpeta para guardar los resultados de la deteccion\n",
        "min_conf_threshold=0.1   # umbral de confianza\n",
        "\n",
        "\n",
        "image_list = glob.glob(PATH_TO_IMAGES + '/*.jpg') + glob.glob(PATH_TO_IMAGES + '/*.JPG') + glob.glob(PATH_TO_IMAGES + '/*.png') + glob.glob(PATH_TO_IMAGES + '/*.bmp')\n",
        "images_to_test = min(500, len(image_list)) \n",
        "\n",
        "\n",
        "txt_only = True\n",
        "\n",
        "# correr la inferencia\n",
        "print('Starting inference on %d images...' % images_to_test)\n",
        "tflite_detect_images(PATH_TO_MODEL, PATH_TO_IMAGES, PATH_TO_LABELS, min_conf_threshold, images_to_test, PATH_TO_RESULTS, txt_only)\n",
        "print('Finished inferencing!')"
      ],
      "metadata": {
        "id": "mf9RUG4O8y8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, ¡calculemos mAP! con el bloque de codigo siguiente"
      ],
      "metadata": {
        "id": "Ajjapjtk-U0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/mAP\n",
        "!python calculate_map_cartucho.py --labels=/content/labelmap.txt"
      ],
      "metadata": {
        "id": "XL2q5PAH-doi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "La puntuación que muestra al final es la puntuación mAP general de su modelo. Lo ideal es que esté por encima del 50% (0,50). Si no es así, para aumentar la precisión tocaria aumentar la cantidad de iamgenes."
      ],
      "metadata": {
        "id": "usqrH54j-nQS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Paso 8: Descargar Modelo TensorFlow Lite"
      ],
      "metadata": {
        "id": "D14vw4h6_7Kl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecute los dos bloques de codigo siguientes para copiar los archivos labelmap en la carpeta del modelo, comprimirlos en una carpeta zip y descargarlos. La carpeta zip contiene los archivos de modelo y labelmap necesarios para ejecutarlo."
      ],
      "metadata": {
        "id": "VleKz1qQAE-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move labelmap and pipeline config files into TFLite model folder and zip it up\n",
        "!cp /content/labelmap.txt /content/custom_model_lite\n",
        "!cp /content/labelmap.pbtxt /content/custom_model_lite\n",
        "!cp /content/models/mymodel/pipeline_file.config /content/custom_model_lite\n",
        "\n",
        "%cd /content\n",
        "!zip -r custom_model_lite.zip custom_model_lite"
      ],
      "metadata": {
        "id": "RP0GCPNCAXVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.download('/content/custom_model_lite.zip')"
      ],
      "metadata": {
        "id": "uWNj4JnYAnj1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}